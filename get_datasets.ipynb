{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa53b02-2f0a-40bb-ac49-9d769fa960c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import struct\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75167e6b-082d-4fc4-b9ae-8a0d9da2940a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 32002979.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 1232657.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 7352708.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3627981.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "All images shape: (70000, 28, 28)\n",
      "All labels shape: (70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Need to get the 70000x28x28 array for the mnist dataset \n",
    "#Also get the 70000x1 array for the labels\n",
    "\n",
    "# Set up the transform for converting the images to Tensor and normalizing them\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the MNIST dataset using PyTorch\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Extract the images and labels from the datasets\n",
    "train_images = train_dataset.data.numpy()  # Shape: (60000, 28, 28)\n",
    "train_labels = train_dataset.targets.numpy()  # Shape: (60000,)\n",
    "test_images = test_dataset.data.numpy()  # Shape: (10000, 28, 28)\n",
    "test_labels = test_dataset.targets.numpy()  # Shape: (10000,)\n",
    "\n",
    "# Combine the train and test data\n",
    "all_images = np.concatenate((train_images, test_images), axis=0)  # Shape: (70000, 28, 28)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)  # Shape: (70000,)\n",
    "\n",
    "# Print the shape of the arrays to confirm\n",
    "print(\"All images shape:\", all_images.shape)  # (70000, 28, 28)\n",
    "print(\"All labels shape:\", all_labels.shape)  # (70000,)\n",
    "\n",
    "np.save('mnist_files/mnist_img.npy', all_images)\n",
    "np.save('mnist_files/mnist_labels.npy', all_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c00d80-c914-4f32-89f3-c713cfe8bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n",
      "(70000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#now, let's combine the masks for each datalevel into a single 70000x28x28 array\n",
    "save_path = './mask/'\n",
    "for i in range(10, 71, 5): \n",
    "    #make the folder if it doesn't exist\n",
    "    dir_path = Path('./mask/' + str(i) + '/')\n",
    "\n",
    "    # Get list of .npy files\n",
    "    file_list = sorted([f for f in os.listdir(dir_path) if f.endswith('.npy')])\n",
    "\n",
    "    # Function to load one file\n",
    "    def load_one(idx_file):\n",
    "        idx, filename = idx_file\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        array = np.load(filepath)\n",
    "        return idx, array\n",
    "\n",
    "    # Preallocate big array\n",
    "    big_array = np.empty((len(file_list), 28, 28), dtype=np.uint8)\n",
    "\n",
    "    # Parallel loading\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # You can adjust max_workers\n",
    "        for idx, array in executor.map(load_one, enumerate(file_list)):\n",
    "            big_array[idx] = array\n",
    "\n",
    "    print(big_array.shape)  # (70000, 28, 28)\n",
    "    np.save(save_path + 'big_mask_' + str(i), big_array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db02ff5-38eb-4238-95f0-9ad238b6a8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
